# Design Notes 
This document describes the architectural decisions, trade-offs, and design reasoning behind the Universal Website Scraper MVP. It follows the assignment’s evaluation areas and is written to make the reviewer understand why each choice was made.

# Static vs JS Fallback 
The scraper always attempts static HTML fetching first and only falls back to JS rendering when necessary. For every incoming URL, I make an HTTP request using httpx with a browser-like User-Agent and parse the response with BeautifulSoup. From this static HTML I try to extract meta data and sections; then I measure the total length of the extracted text across all sections. If the static fetch fails (due to 4xx/5xx or network issues) or if the total text length is below a small threshold, I treat the page as likely JS-heavy or incomplete and switch to Playwright. In the JS fallback path, I open a headless Chromium browser, navigate to the same URL with wait_until="networkidle", perform scroll and click interactions, and then re-run the same section extraction logic on the fully rendered DOM. This static-first, JS-fallback strategy keeps simple pages fast while still handling modern, JavaScript-driven sites.

# Wait Strategy for JS 
For JavaScript-rendered pages, the scraper relies primarily on network idle as the initial wait condition. When navigating to a URL in Playwright, the browser loads the page with wait_until="networkidle", ensuring that most JavaScript execution and network requests have settled. After this main wait, I use short fixed sleep intervals following each scroll and click interaction. These brief pauses allow any lazily loaded or dynamically appended content—such as images, infinite scroll items, or elements revealed by “Load more” buttons—to fully render before extracting the DOM. I do not perform explicit selector-based waits in this MVP; instead, the combination of network idle and small timed waits provides stable and predictable results while keeping the implementation simple.

# Click & Scroll Strategy 
The scraper implements a minimal but effective interaction flow to expose dynamic content during JavaScript rendering. For click interactions, it specifically looks for common UI patterns such as “Load more” or “Show more” buttons, which frequently appear on feed-based or content-heavy pages. If such an element is found, the scraper clicks the first matching button and waits briefly to allow additional items to load. Beyond clicking, the scraper performs three sequential scroll operations, each followed by a short delay. This scroll depth satisfies the assignment requirement of reaching at least three loads and effectively handles infinite scroll–style pagination. To prevent unnecessary or excessive automation, the system enforces simple stop conditions: a maximum of one click attempt on load-more elements, three scroll iterations, and Playwright’s built-in navigation timeout. Together, these measures provide enough interaction to reveal additional content while keeping the logic predictable and lightweight for an MVP.

# Section Grouping & Labels 
The scraper organizes the DOM into meaningful sections by relying on semantic HTML structure. It first identifies a top-level container—preferably the <main> element, or <body> if <main> is unavailable—and then groups content based on major structural tags within that container, such as <header>, <nav>, <section>, <article>, and <footer>. Each of these blocks is treated as an independent section and is parsed for headings, text, links, images, lists, and tables. The section type is derived directly from the underlying HTML tag: headers become “hero,” navigation bars become “nav,” footers remain “footer,” and all other structural elements are classified as general “section” types. Labels are generated using the first heading within the block when available; if no heading exists, the scraper falls back to the first few words of the block’s text content. This strategy keeps labels human-readable while maintaining predictable, structured grouping of the page’s content.

# Noise Filtering & Truncation 
Noise filtering in this MVP is intentionally minimal to avoid unintentionally removing meaningful content across different site designs. The scraper does not attempt to explicitly filter out cookie banners, overlays, advertisements, or other UI elements, since these vary widely and require more advanced heuristics or selector-based rules. Instead, the focus is on preserving the page’s structural integrity while ensuring that the JSON response remains manageable. To control payload size, each section includes a rawHtml field that is capped at a maximum of 5,000 characters. When the raw HTML exceeds this limit, it is truncated and a truncated: true flag is set to indicate this reduction. This approach keeps responses lightweight and predictable, while still giving users access to enough underlying markup for debugging or deeper inspection.
